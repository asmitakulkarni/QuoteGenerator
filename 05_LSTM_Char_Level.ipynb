{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io, os\n",
    "import pickle\n",
    "import string\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues_dict = pickle.load(open('dialogues.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "harry_dialogue = dialogues_dict['HARRY'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harry_dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "harry_corpus = ' '.join(harry_dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harry_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 58699\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(harry_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(harry_corpus)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  58699\n",
      "Total Vocab:  75\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(harry_corpus)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  58599\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = harry_corpus[i:i + seq_length]\n",
    "    seq_out = harry_corpus[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/metis/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/metis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"lstm1-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/metis/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 3.2251\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.22513, saving model to lstm-weights-improvement-01-3.2251.hdf5\n",
      "Epoch 2/100\n",
      "58599/58599 [==============================] - 278s 5ms/step - loss: 3.0440\n",
      "\n",
      "Epoch 00002: loss improved from 3.22513 to 3.04403, saving model to lstm-weights-improvement-02-3.0440.hdf5\n",
      "Epoch 3/100\n",
      "58599/58599 [==============================] - 273s 5ms/step - loss: 2.9447\n",
      "\n",
      "Epoch 00003: loss improved from 3.04403 to 2.94472, saving model to lstm-weights-improvement-03-2.9447.hdf5\n",
      "Epoch 4/100\n",
      "58599/58599 [==============================] - 274s 5ms/step - loss: 2.8916\n",
      "\n",
      "Epoch 00004: loss improved from 2.94472 to 2.89161, saving model to lstm-weights-improvement-04-2.8916.hdf5\n",
      "Epoch 5/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.8502\n",
      "\n",
      "Epoch 00005: loss improved from 2.89161 to 2.85025, saving model to lstm-weights-improvement-05-2.8502.hdf5\n",
      "Epoch 6/100\n",
      "58599/58599 [==============================] - 274s 5ms/step - loss: 2.8191\n",
      "\n",
      "Epoch 00006: loss improved from 2.85025 to 2.81909, saving model to lstm-weights-improvement-06-2.8191.hdf5\n",
      "Epoch 7/100\n",
      "58599/58599 [==============================] - 275s 5ms/step - loss: 2.7962\n",
      "\n",
      "Epoch 00007: loss improved from 2.81909 to 2.79621, saving model to lstm-weights-improvement-07-2.7962.hdf5\n",
      "Epoch 8/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.7730\n",
      "\n",
      "Epoch 00008: loss improved from 2.79621 to 2.77303, saving model to lstm-weights-improvement-08-2.7730.hdf5\n",
      "Epoch 9/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 2.7523\n",
      "\n",
      "Epoch 00009: loss improved from 2.77303 to 2.75225, saving model to lstm-weights-improvement-09-2.7523.hdf5\n",
      "Epoch 10/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.7308\n",
      "\n",
      "Epoch 00010: loss improved from 2.75225 to 2.73083, saving model to lstm-weights-improvement-10-2.7308.hdf5\n",
      "Epoch 11/100\n",
      "58599/58599 [==============================] - 275s 5ms/step - loss: 2.7111\n",
      "\n",
      "Epoch 00011: loss improved from 2.73083 to 2.71115, saving model to lstm-weights-improvement-11-2.7111.hdf5\n",
      "Epoch 12/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 2.6894\n",
      "\n",
      "Epoch 00012: loss improved from 2.71115 to 2.68939, saving model to lstm-weights-improvement-12-2.6894.hdf5\n",
      "Epoch 13/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 2.6661\n",
      "\n",
      "Epoch 00013: loss improved from 2.68939 to 2.66607, saving model to lstm-weights-improvement-13-2.6661.hdf5\n",
      "Epoch 14/100\n",
      "58599/58599 [==============================] - 274s 5ms/step - loss: 2.6426\n",
      "\n",
      "Epoch 00014: loss improved from 2.66607 to 2.64258, saving model to lstm-weights-improvement-14-2.6426.hdf5\n",
      "Epoch 15/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 2.6198\n",
      "\n",
      "Epoch 00015: loss improved from 2.64258 to 2.61983, saving model to lstm-weights-improvement-15-2.6198.hdf5\n",
      "Epoch 16/100\n",
      "58599/58599 [==============================] - 274s 5ms/step - loss: 2.5933\n",
      "\n",
      "Epoch 00016: loss improved from 2.61983 to 2.59332, saving model to lstm-weights-improvement-16-2.5933.hdf5\n",
      "Epoch 17/100\n",
      "58599/58599 [==============================] - 275s 5ms/step - loss: 2.5706\n",
      "\n",
      "Epoch 00017: loss improved from 2.59332 to 2.57061, saving model to lstm-weights-improvement-17-2.5706.hdf5\n",
      "Epoch 18/100\n",
      "58599/58599 [==============================] - 275s 5ms/step - loss: 2.5374\n",
      "\n",
      "Epoch 00018: loss improved from 2.57061 to 2.53736, saving model to lstm-weights-improvement-18-2.5374.hdf5\n",
      "Epoch 19/100\n",
      "58599/58599 [==============================] - 275s 5ms/step - loss: 2.5091\n",
      "\n",
      "Epoch 00019: loss improved from 2.53736 to 2.50906, saving model to lstm-weights-improvement-19-2.5091.hdf5\n",
      "Epoch 20/100\n",
      "58599/58599 [==============================] - 275s 5ms/step - loss: 2.4765\n",
      "\n",
      "Epoch 00020: loss improved from 2.50906 to 2.47648, saving model to lstm-weights-improvement-20-2.4765.hdf5\n",
      "Epoch 21/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.4431\n",
      "\n",
      "Epoch 00021: loss improved from 2.47648 to 2.44307, saving model to lstm-weights-improvement-21-2.4431.hdf5\n",
      "Epoch 22/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.4106\n",
      "\n",
      "Epoch 00022: loss improved from 2.44307 to 2.41057, saving model to lstm-weights-improvement-22-2.4106.hdf5\n",
      "Epoch 23/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.3796\n",
      "\n",
      "Epoch 00023: loss improved from 2.41057 to 2.37963, saving model to lstm-weights-improvement-23-2.3796.hdf5\n",
      "Epoch 24/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.3460\n",
      "\n",
      "Epoch 00024: loss improved from 2.37963 to 2.34597, saving model to lstm-weights-improvement-24-2.3460.hdf5\n",
      "Epoch 25/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.3139\n",
      "\n",
      "Epoch 00025: loss improved from 2.34597 to 2.31385, saving model to lstm-weights-improvement-25-2.3139.hdf5\n",
      "Epoch 26/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.2770\n",
      "\n",
      "Epoch 00026: loss improved from 2.31385 to 2.27695, saving model to lstm-weights-improvement-26-2.2770.hdf5\n",
      "Epoch 27/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.2472\n",
      "\n",
      "Epoch 00027: loss improved from 2.27695 to 2.24719, saving model to lstm-weights-improvement-27-2.2472.hdf5\n",
      "Epoch 28/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 2.2165\n",
      "\n",
      "Epoch 00028: loss improved from 2.24719 to 2.21655, saving model to lstm-weights-improvement-28-2.2165.hdf5\n",
      "Epoch 29/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.1884\n",
      "\n",
      "Epoch 00029: loss improved from 2.21655 to 2.18841, saving model to lstm-weights-improvement-29-2.1884.hdf5\n",
      "Epoch 30/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.1506\n",
      "\n",
      "Epoch 00030: loss improved from 2.18841 to 2.15062, saving model to lstm-weights-improvement-30-2.1506.hdf5\n",
      "Epoch 31/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.1220\n",
      "\n",
      "Epoch 00031: loss improved from 2.15062 to 2.12197, saving model to lstm-weights-improvement-31-2.1220.hdf5\n",
      "Epoch 32/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.0942\n",
      "\n",
      "Epoch 00032: loss improved from 2.12197 to 2.09418, saving model to lstm-weights-improvement-32-2.0942.hdf5\n",
      "Epoch 33/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.0741\n",
      "\n",
      "Epoch 00033: loss improved from 2.09418 to 2.07408, saving model to lstm-weights-improvement-33-2.0741.hdf5\n",
      "Epoch 34/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 2.0368\n",
      "\n",
      "Epoch 00034: loss improved from 2.07408 to 2.03675, saving model to lstm-weights-improvement-34-2.0368.hdf5\n",
      "Epoch 35/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.0150\n",
      "\n",
      "Epoch 00035: loss improved from 2.03675 to 2.01497, saving model to lstm-weights-improvement-35-2.0150.hdf5\n",
      "Epoch 36/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.9885\n",
      "\n",
      "Epoch 00036: loss improved from 2.01497 to 1.98847, saving model to lstm-weights-improvement-36-1.9885.hdf5\n",
      "Epoch 37/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.9611\n",
      "\n",
      "Epoch 00037: loss improved from 1.98847 to 1.96109, saving model to lstm-weights-improvement-37-1.9611.hdf5\n",
      "Epoch 38/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.9376\n",
      "\n",
      "Epoch 00038: loss improved from 1.96109 to 1.93758, saving model to lstm-weights-improvement-38-1.9376.hdf5\n",
      "Epoch 39/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.9148\n",
      "\n",
      "Epoch 00039: loss improved from 1.93758 to 1.91475, saving model to lstm-weights-improvement-39-1.9148.hdf5\n",
      "Epoch 40/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.8909\n",
      "\n",
      "Epoch 00040: loss improved from 1.91475 to 1.89087, saving model to lstm-weights-improvement-40-1.8909.hdf5\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.8723\n",
      "\n",
      "Epoch 00041: loss improved from 1.89087 to 1.87233, saving model to lstm-weights-improvement-41-1.8723.hdf5\n",
      "Epoch 42/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.8515\n",
      "\n",
      "Epoch 00042: loss improved from 1.87233 to 1.85147, saving model to lstm-weights-improvement-42-1.8515.hdf5\n",
      "Epoch 43/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.8342\n",
      "\n",
      "Epoch 00043: loss improved from 1.85147 to 1.83420, saving model to lstm-weights-improvement-43-1.8342.hdf5\n",
      "Epoch 44/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.8114\n",
      "\n",
      "Epoch 00044: loss improved from 1.83420 to 1.81137, saving model to lstm-weights-improvement-44-1.8114.hdf5\n",
      "Epoch 45/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.7812\n",
      "\n",
      "Epoch 00045: loss improved from 1.81137 to 1.78121, saving model to lstm-weights-improvement-45-1.7812.hdf5\n",
      "Epoch 46/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.7705\n",
      "\n",
      "Epoch 00046: loss improved from 1.78121 to 1.77051, saving model to lstm-weights-improvement-46-1.7705.hdf5\n",
      "Epoch 47/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.7466\n",
      "\n",
      "Epoch 00047: loss improved from 1.77051 to 1.74662, saving model to lstm-weights-improvement-47-1.7466.hdf5\n",
      "Epoch 48/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.7324\n",
      "\n",
      "Epoch 00048: loss improved from 1.74662 to 1.73236, saving model to lstm-weights-improvement-48-1.7324.hdf5\n",
      "Epoch 49/100\n",
      "58599/58599 [==============================] - 278s 5ms/step - loss: 1.7206\n",
      "\n",
      "Epoch 00049: loss improved from 1.73236 to 1.72057, saving model to lstm-weights-improvement-49-1.7206.hdf5\n",
      "Epoch 50/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.7014\n",
      "\n",
      "Epoch 00050: loss improved from 1.72057 to 1.70141, saving model to lstm-weights-improvement-50-1.7014.hdf5\n",
      "Epoch 51/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.6823\n",
      "\n",
      "Epoch 00051: loss improved from 1.70141 to 1.68228, saving model to lstm-weights-improvement-51-1.6823.hdf5\n",
      "Epoch 52/100\n",
      "58599/58599 [==============================] - 278s 5ms/step - loss: 1.6689\n",
      "\n",
      "Epoch 00052: loss improved from 1.68228 to 1.66889, saving model to lstm-weights-improvement-52-1.6689.hdf5\n",
      "Epoch 53/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.6623\n",
      "\n",
      "Epoch 00053: loss improved from 1.66889 to 1.66229, saving model to lstm-weights-improvement-53-1.6623.hdf5\n",
      "Epoch 54/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.6455\n",
      "\n",
      "Epoch 00054: loss improved from 1.66229 to 1.64552, saving model to lstm-weights-improvement-54-1.6455.hdf5\n",
      "Epoch 55/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.6226\n",
      "\n",
      "Epoch 00055: loss improved from 1.64552 to 1.62261, saving model to lstm-weights-improvement-55-1.6226.hdf5\n",
      "Epoch 56/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.6174\n",
      "\n",
      "Epoch 00056: loss improved from 1.62261 to 1.61743, saving model to lstm-weights-improvement-56-1.6174.hdf5\n",
      "Epoch 57/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.6002\n",
      "\n",
      "Epoch 00057: loss improved from 1.61743 to 1.60019, saving model to lstm-weights-improvement-57-1.6002.hdf5\n",
      "Epoch 58/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.5900\n",
      "\n",
      "Epoch 00058: loss improved from 1.60019 to 1.59000, saving model to lstm-weights-improvement-58-1.5900.hdf5\n",
      "Epoch 59/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.5929\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.59000\n",
      "Epoch 60/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.5588\n",
      "\n",
      "Epoch 00060: loss improved from 1.59000 to 1.55876, saving model to lstm-weights-improvement-60-1.5588.hdf5\n",
      "Epoch 61/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.5817\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.55876\n",
      "Epoch 62/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.5484\n",
      "\n",
      "Epoch 00062: loss improved from 1.55876 to 1.54844, saving model to lstm-weights-improvement-62-1.5484.hdf5\n",
      "Epoch 63/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.5308\n",
      "\n",
      "Epoch 00063: loss improved from 1.54844 to 1.53085, saving model to lstm-weights-improvement-63-1.5308.hdf5\n",
      "Epoch 64/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.5686\n",
      "\n",
      "Epoch 00064: loss did not improve from 1.53085\n",
      "Epoch 65/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.5340\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.53085\n",
      "Epoch 66/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.5215\n",
      "\n",
      "Epoch 00066: loss improved from 1.53085 to 1.52147, saving model to lstm-weights-improvement-66-1.5215.hdf5\n",
      "Epoch 67/100\n",
      "58599/58599 [==============================] - 278s 5ms/step - loss: 1.4954\n",
      "\n",
      "Epoch 00067: loss improved from 1.52147 to 1.49543, saving model to lstm-weights-improvement-67-1.4954.hdf5\n",
      "Epoch 68/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.4802\n",
      "\n",
      "Epoch 00068: loss improved from 1.49543 to 1.48020, saving model to lstm-weights-improvement-68-1.4802.hdf5\n",
      "Epoch 69/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.4785\n",
      "\n",
      "Epoch 00069: loss improved from 1.48020 to 1.47851, saving model to lstm-weights-improvement-69-1.4785.hdf5\n",
      "Epoch 70/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.4686\n",
      "\n",
      "Epoch 00070: loss improved from 1.47851 to 1.46856, saving model to lstm-weights-improvement-70-1.4686.hdf5\n",
      "Epoch 71/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.4696\n",
      "\n",
      "Epoch 00071: loss did not improve from 1.46856\n",
      "Epoch 72/100\n",
      "58599/58599 [==============================] - 278s 5ms/step - loss: 1.4579\n",
      "\n",
      "Epoch 00072: loss improved from 1.46856 to 1.45786, saving model to lstm-weights-improvement-72-1.4579.hdf5\n",
      "Epoch 73/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.4310\n",
      "\n",
      "Epoch 00073: loss improved from 1.45786 to 1.43102, saving model to lstm-weights-improvement-73-1.4310.hdf5\n",
      "Epoch 74/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.5072\n",
      "\n",
      "Epoch 00074: loss did not improve from 1.43102\n",
      "Epoch 75/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.4773\n",
      "\n",
      "Epoch 00075: loss did not improve from 1.43102\n",
      "Epoch 76/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.4212\n",
      "\n",
      "Epoch 00076: loss improved from 1.43102 to 1.42123, saving model to lstm-weights-improvement-76-1.4212.hdf5\n",
      "Epoch 77/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.7095\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.42123\n",
      "Epoch 78/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.4205\n",
      "\n",
      "Epoch 00078: loss improved from 1.42123 to 1.42053, saving model to lstm-weights-improvement-78-1.4205.hdf5\n",
      "Epoch 79/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.3819\n",
      "\n",
      "Epoch 00079: loss improved from 1.42053 to 1.38188, saving model to lstm-weights-improvement-79-1.3819.hdf5\n",
      "Epoch 80/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.3856\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.38188\n",
      "Epoch 81/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.3781\n",
      "\n",
      "Epoch 00081: loss improved from 1.38188 to 1.37814, saving model to lstm-weights-improvement-81-1.3781.hdf5\n",
      "Epoch 82/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.3837\n",
      "\n",
      "Epoch 00082: loss did not improve from 1.37814\n",
      "Epoch 83/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.3966\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.37814\n",
      "Epoch 84/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.4989\n",
      "\n",
      "Epoch 00084: loss did not improve from 1.37814\n",
      "Epoch 85/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.3915\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.37814\n",
      "Epoch 86/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.3748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: loss improved from 1.37814 to 1.37475, saving model to lstm-weights-improvement-86-1.3748.hdf5\n",
      "Epoch 87/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 2.0123\n",
      "\n",
      "Epoch 00087: loss did not improve from 1.37475\n",
      "Epoch 88/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.4417\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.37475\n",
      "Epoch 89/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.3724\n",
      "\n",
      "Epoch 00089: loss improved from 1.37475 to 1.37235, saving model to lstm-weights-improvement-89-1.3724.hdf5\n",
      "Epoch 90/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.3944\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.37235\n",
      "Epoch 91/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.3678\n",
      "\n",
      "Epoch 00091: loss improved from 1.37235 to 1.36779, saving model to lstm-weights-improvement-91-1.3678.hdf5\n",
      "Epoch 92/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.3391\n",
      "\n",
      "Epoch 00092: loss improved from 1.36779 to 1.33907, saving model to lstm-weights-improvement-92-1.3391.hdf5\n",
      "Epoch 93/100\n",
      "58599/58599 [==============================] - 277s 5ms/step - loss: 1.3477\n",
      "\n",
      "Epoch 00093: loss did not improve from 1.33907\n",
      "Epoch 94/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.3478\n",
      "\n",
      "Epoch 00094: loss did not improve from 1.33907\n",
      "Epoch 95/100\n",
      "58599/58599 [==============================] - 276s 5ms/step - loss: 1.3419\n",
      "\n",
      "Epoch 00095: loss did not improve from 1.33907\n",
      "Epoch 96/100\n",
      "58599/58599 [==============================] - 280s 5ms/step - loss: 1.3826\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.33907\n",
      "Epoch 97/100\n",
      "58599/58599 [==============================] - 281s 5ms/step - loss: 1.3204\n",
      "\n",
      "Epoch 00097: loss improved from 1.33907 to 1.32043, saving model to lstm-weights-improvement-97-1.3204.hdf5\n",
      "Epoch 98/100\n",
      "58599/58599 [==============================] - 280s 5ms/step - loss: 1.3205\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.32043\n",
      "Epoch 99/100\n",
      "58599/58599 [==============================] - 282s 5ms/step - loss: 1.3403\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.32043\n",
      "Epoch 100/100\n",
      "58599/58599 [==============================] - 278s 5ms/step - loss: 1.3198\n",
      "\n",
      "Epoch 00100: loss improved from 1.32043 to 1.31976, saving model to lstm-weights-improvement-100-1.3198.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb27e491d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "\n",
    "# filename = \"lstm5-weights-improvement-100-0.9618.hdf5\"\n",
    "filename = \"lstm-weights-improvement-97-1.3204.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      " the mone me. Aruule me?  Mow. Wou ie wou teve g momstlde. What sas the blondr oet of to poen it? Wh\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "# print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "eng hn the gnhwt dfd io h mere of fy. Iemei aetees. Hhmk.  I’le oe.  Tpa iir ii atis. B'll gor oh io  - dr toe sisce ie tio erenr, Si Saareod wharl aon mnr-  Sle ttel iy whi coa’e carel bei?  Yoeyre hoc gblo! oom? Sho ann tha semeo Demmid thoti fr I dos Bro, H doswe.. W aldles har.  Bkb yhe hldd o. Bed I gonit temen bhr on. Ohef im hnd. .Ol. Mharey!they'd whst yiu eew i wortire,  Ms mo meoe. Teathm ihn?  Det oome se hid tosesls? Tolhksir hnint'  Tnlker’e tsy’rel,s cr toe pafe afyuyed tork soueeg\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "# print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "#     index = np.argmax(prediction)\n",
    "    index = np.random.choice(len(prediction[0]),p=prediction[0])\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
